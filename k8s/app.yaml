apiVersion: v1
kind: Namespace
metadata:
  name: interop
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: convo-db
  namespace: interop
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: interop
data:
  PORT: "3000"
  DB_PATH: "/data/data.db"
  DEFAULT_LLM_PROVIDER: "openrouter"
  DEFAULT_LLM_MODEL: "openai/gpt-oss-120b:nitro"
  # Public config exported into built frontends at container startup
  PUBLIC_API_BASE_URL: "https://chitchat.fhir.me/api"
  # Note: Set OPENROUTER_API_KEY in Secret `app-secrets`
---
apiVersion: v1
kind: Secret
metadata:
  name: app-secrets
  namespace: interop
type: Opaque
stringData:
  # Set if you use real providers; otherwise leave empty
  GEMINI_API_KEY: ""
  OPENROUTER_API_KEY: ""  # paste your key here before applying, or set it out-of-band
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: interop-api
  namespace: interop
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: interop-api
  template:
    metadata:
      labels:
        app: interop-api
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
        fsGroup: 10001
        fsGroupChangePolicy: OnRootMismatch
      containers:
        - name: api
          image: ghcr.io/jmandel/conversational-interop:main
          imagePullPolicy: Always
          ports:
            - containerPort: 3000
              name: http
          volumeMounts:
            - name: data
              mountPath: /data
            - name: public
              mountPath: /app/public
          envFrom:
            - configMapRef:
                name: app-config
            - secretRef:
                name: app-secrets
          env:
            - name: DEBUG_LLM_REQUESTS
              value: "0" # set to "1" or "true" to enable plain-text LLM request/response logs
            - name: LLM_DEBUG_DIR
              value: "/data/llm-debug"
            # Limit OpenRouter models to a curated subset (optional)
            - name: LLM_MODELS_OPENROUTER_INCLUDE
              value: "openai/gpt-oss-120b:nitro,qwen/qwen3-235b-a22b-2507:nitro"
          readinessProbe:
            httpGet:
              path: /api/health
              port: http
            initialDelaySeconds: 2
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /api/health
              port: http
            initialDelaySeconds: 5
            periodSeconds: 10
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: convo-db
        - name: public
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: interop-api
  namespace: interop
spec:
  selector:
    app: interop-api
  ports:
    - name: http
      port: 80
      targetPort: http
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: interop-api
  namespace: interop
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
spec:
  # Match working namespaces: rely on kubernetes.io/ingress.class annotation
  tls:
    - hosts:
        - chitchat.fhir.me
      secretName: chitchat-fhir-me-tls
  rules:
    - host: chitchat.fhir.me
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: interop-api
                port:
                  number: 80
---
# Explicit Certificate to trigger issuance
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: chitchat-fhir-me
  namespace: interop
spec:
  secretName: chitchat-fhir-me-tls
  commonName: chitchat.fhir.me
  dnsNames:
    - chitchat.fhir.me
  issuerRef:
    kind: ClusterIssuer
    name: letsencrypt-prod

